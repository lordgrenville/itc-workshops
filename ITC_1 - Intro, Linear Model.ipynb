{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ITC_1 - Intro, Linear Model.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"}},"cells":[{"metadata":{"id":"rgq8gVu5eiQ9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Israei Tech Challenge - Part 1 of 4\n","# Welcome to the ITC Taboola workshop. In this workshop we will address a real world problem:\n","# We have a list of users and items (ads) and their features. \n","# Lets try to predict the probablilty of a click (a user clicking on the commercial). \n","\n","# We are going to open the data, get to know it a little bit, then do a basic model in tensor flow. \n","\n","# Let's start with basic imports. \n","\n","import pandas as pd \n","import numpy as np\n","from collections import Counter, defaultdict\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z1Wjf2BLeiRL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Read the files\n","# These are real users and real advertisment from a single site.\n","# Start with 'ITC_20K.csv' (10MB), and later you can move to the ITC_40K.csv (20MB).\n","\n","data = pd.read_csv('ITC_20K.csv')\n","\n","# Basic clean-up \n","data.replace('', np.nan, inplace=True)\n","data = data.dropna()\n","\n","# Look at the data columns. Do you understand what they mean?  \n","data.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hQDWBX8BeiRV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Try to understand the meaning of each column\n","# Separate columns to user features and source/context/publisher features\n","\n","source_features = \"source_id,content_category,ad_type,quality_level,source_item_type,syndicator_id,\\\n","                    target_id,campaign_id,title,campaign_language\".split(\",\")\n","user_features = \"user_id,browser_platform,os_family,country_code,os_name,country,region,browser_name,\\\n","                 user_clicks,user_recs,prev_syndicator_clicks,target_recs,campaign_recs,\\\n","                 user_category_clicks,user_category_recs\".split(\",\")\n","\n","label = \"is_click\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"uiqRBqW2eiRg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Our machine learning algorithm will handle each column according to his type. \n","# Task: Separate columns to lists of numeric and categorical features. notice that even columns that are numbers\n","# may be categorical features, like _id features. \n","# Notice we have numerical features labeled as objects since they are arrays! Let's leave them out for now. \n","\n","numeric_features = ...\n","categorical_features = ..."],"execution_count":0,"outputs":[]},{"metadata":{"id":"8oSAOedqeiRo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# A basic check with data.dtypes shows that the python read_csv may read the format wrongfully\n","print data.dtypes\n","\n","# To make sure all the numeric data is formated, lets do a basic clean-up of numerical data\n","data[numeric_features] = data[numeric_features].apply(pd.to_numeric, errors='coerce')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AI5V_xVieiRt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Let's answer a few basic questions before we start: \n","\n","# Task: How big is our data set? \n","data_size = ...\n","total_num_features = ...\n","\n","print \"number of samples: \"\n","print data_size\n","print \"number of features: \" \n","total_num_features"],"execution_count":0,"outputs":[]},{"metadata":{"id":"58TLDjIyeiR1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Task: How many people clicked / haven't clicked? \n","\n","num_click = ...\n","num_no_click = ...\n","\n","print \"clicked: \" + str(num_click)\n","print \"haven't clicked: \" + str(num_no_click)\n","print \"clicked_ratio: \" + str(num_click/(num_no_click+num_click))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UzJIlJ3veiR7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# From what browser people are coming for this publisher? \n","# Task: Plot or count. Is this data usual? "],"execution_count":0,"outputs":[]},{"metadata":{"id":"IJr6wBFseiSD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Lets try to catch some interesting \"signal\". \n","# It is always helpful to to plot the data before starting. \n","# Try to find some connection between our numerical features and the \"is_click\" columns. \n","# If you're going to use scatter, it is sometimes easier to add noise to the is_click column,\n","# to help see the spread of the data. \n","# This MIGHT help: \n","# noise = np.random.randn(data_size)/10\n","# is_clicked = data[\"is_click\"].values + noise"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LfxMg_ZVeiSL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Enough exploration! \n","# Let's try to build our first model: A simple neural net, only using our numerical features. \n","\n","# Only numercial features\n","only_num_data = data[numeric_features].dropna()\n","list(only_num_data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u6asqq1LeiSY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Task: Create msk to split into train, test sets\n","msk = ... "],"execution_count":0,"outputs":[]},{"metadata":{"id":"xIJmAY-7eiSf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# train, test split\n","train = only_num_data[msk]\n","test = only_num_data[~msk]\n","\n","# Separate the label columns from our features\n","y_train = train[\"is_click\"]\n","del train[\"is_click\"]\n","y_test = test[\"is_click\"]\n","del test[\"is_click\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xu683rzSeiSl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Let's start with linear regression with TensorFlow. \n","# We are going to use the 5 numeric features we've gotten to know.\n","# This code is roughly based on: \n","# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb\n","\n","# Important parameters for the model\n","# You can play with these, as you wish. \n","\n","num_samples = train.shape[0]\n","num_features = train.shape[1] \n","batch_size = 50\n","training_epochs = 5\n","total_batch = int(num_samples/batch_size)\n","learning_rate = 0.08\n","print_every = 1 # epochs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QhV0729neiSt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Lets build our TF Graph step-by-step"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tGjOEPq9eiSz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# TF Graph Input\n","\n","# Dataset of features\n","x = tf.placeholder(tf.float32, [None, num_features])\n","\n","# Labels\n","y = tf.placeholder(tf.float32, [None, 1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XD9Py9HueiS3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Set model variable (= what the model is going to learn)\n","\n","W = tf.Variable(tf.random_normal([num_features, 1], stddev=0.15), name=\"weights\")\n","b = tf.Variable(tf.zeros([1]), name=\"bias\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xVB_LdKaeiS8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Create the model operations. \n","# These are different calculations of the placeholder and variables. \n","\n","# Initialize the variables \n","init = tf.global_variables_initializer()\n","\n","# Construct model\n","pred = tf.sigmoid(tf.matmul(x, W) + b)\n","\n","# Minimize error using MSE\n","cost = tf.losses.mean_squared_error(labels = y, predictions = pred)\n","\n","# Gradient Descent\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wCCXjcVDeiTM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Start TensorFlow learning session. \n","\n","# Start training\n","def train_session(train, test, y_train, y_test):\n","    with tf.Session() as sess:\n","\n","        # Run the initiation operation (initializer)\n","        sess.run(init)\n","\n","        # Training cycle\n","        for epoch in range(training_epochs):\n","            avg_cost = 0.\n","\n","            # Loop over all batches\n","            for i in range(total_batch):\n","\n","                # Build inputs in every batch loop\n","                batch_xs = train.iloc[i*batch_size : (i+1)*batch_size].values   \n","                batch_ys = y_train.iloc[i*batch_size : (i+1)*batch_size]\n","                batch_ys = batch_ys.values.reshape([batch_size,1])\n","\n","                # Run optimization operation (backprop) and cost operation (to get loss value)\n","                _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys})\n","\n","                # Compute average loss\n","                avg_cost += c / (1.0*total_batch)\n","\n","            # Display logs per epoch step\n","            if (epoch+1) % print_every == 0:\n","                 print(\"Epoch:\", '%02d' % (epoch+1), \"cost=\", avg_cost)\n","\n","        print(\"Optimization Finished!\")\n","\n","        # Test model\n","        correct_prediction = tf.equal(tf.round(pred), y)\n","        # Calculate accuracy\n","        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","        batch_xs = test.values            \n","        batch_ys = y_test\n","        batch_ys = batch_ys.values.reshape([len(y_test),1])\n","        print(\"Accuracy:\", accuracy.eval({x: batch_xs, y: batch_ys}))     "],"execution_count":0,"outputs":[]},{"metadata":{"id":"7dhAzk8NeiTd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["train_session(train, test, y_train, y_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9EuFiV30eiTo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# The results should be about 48% \n","# This is VERY low. \n","# What are we missing ?\n","# Try again with using normalization on the numeric columns. \n","# Why should this help ? "],"execution_count":0,"outputs":[]},{"metadata":{"id":"YuVLEhJ6eiTt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def normalize(df):\n","    result = df.copy()\n","    for feature_name in df.columns:\n","        max_value = df[feature_name].max()\n","        min_value = df[feature_name].min()\n","        if (max_value - min_value) > 0:\n","            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n","    return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ntg3nzVceiT7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["train = normalize(train)\n","test = normalize(test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EwBs4rhieiUB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Re-run the training\n","# Exactly the same training from before! We are using the exact same graph with a different input.\n","train_session(train, test, y_train, y_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E9F4CdapeiUH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Almost 55% - definetly a bit better!\n","# We're going to add 2 columns of numerical features to our data and do the calculation once again:\n","# These features are past_user_category_recs and past_user_category_clicks\n","# We have to parse these with there two ready functions: \n","\n","# A function that takes in a single cell that has array value and \n","# makes sure all arrays are the same length = max_values\n","def split_cell_to_list(x, max_values=24, pad_value=[0], value_type='float'):\n","    if type(x)==str: \n","        x = x.split(' ')\n","        if len(x) >= max_values:\n","            x = x[:max_values]\n","        else: \n","            x = x + pad_value*(max_values-len(x))\n","        return x\n","    else: \n","        print 'cell with bad content'\n","        print type(x),x\n","\n","# A function that receives a dataframe and a column_name and explodes \n","# that column to num_columns different columns          \n","def parse_and_add_columns(df, column_name):\n","    if column_name in list(df):\n","        df[column_name] = df[column_name].apply(split_cell_to_list, args=())   \n","        temp = pd.DataFrame(df[column_name].values.tolist())\n","        temp = temp.rename(columns=lambda x: column_name+str(x))\n","        df = pd.concat([df, temp], axis=1, join='inner')\n","        del df[column_name]\n","    else: \n","        print 'columns already parsed!'\n","    return df"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NRGP37PEeiUK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# two more columns\n","numeric_features = \"user_recs,prev_syndicator_clicks,target_recs,campaign_recs,user_clicks,\\\n","user_category_clicks,user_category_recs,is_click\".split(\",\") \n","only_num_data = data[numeric_features]\n","\n","# print the data set before parsing\n","only_num_data.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5T8hIRLOeiUN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# parse data with given function \"parse_and_add_columns\"\n","\n","only_num_data = parse_and_add_columns(only_num_data, 'user_category_recs')\n","only_num_data = parse_and_add_columns(only_num_data, 'user_category_clicks')\n","\n","only_num_data.head()\n","\n","# How many new features did we get? "],"execution_count":0,"outputs":[]},{"metadata":{"id":"boTCe2d1eiUV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Split into train, test sets\n","only_num_data = only_num_data.apply(pd.to_numeric, errors='coerce')\n","\n","msk = np.random.rand(len(only_num_data)) < 0.8 \n","train = only_num_data[msk].dropna()\n","test = only_num_data[~msk].dropna()\n","\n","train = normalize(train)\n","test = normalize(test)\n","\n","# Create label\n","y_train = train[\"is_click\"]\n","del train[\"is_click\"]\n","y_test = test[\"is_click\"]\n","del test[\"is_click\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kk86hpkFeiUc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# we need to restart our graph, because now we have a different num_features -> \n","# Some of our variables are going to have different sizes\n","\n","tf.reset_default_graph()\n","\n","# Exactly the code from before! BUT we need to run it again since num_features is different now. \n","\n","num_features = len(list(train))\n","num_samples = train.shape[0]\n","batch_size = 50\n","training_epochs=13\n","total_batch = int(num_samples/batch_size)\n","learning_rate = 0.8\n","\n","# tf Graph Input\n","x = tf.placeholder(tf.float32, [None, num_features])\n","y = tf.placeholder(tf.float32, [None, 1])\n","\n","# Set model weights\n","# W = tf.Variable(tf.zeros([num_features, 1]), name=\"weight\")\n","W = tf.Variable(tf.random_normal([num_features, 1], stddev=0.05), name=\"weights\")\n","b = tf.Variable(tf.zeros([1]), name=\"bias\")\n","\n","# Construct model\n","matmul = tf.matmul(x, W)\n","pred = tf.sigmoid(matmul + b)\n","\n","# Minimize error using cross entropy\n","cost = tf.losses.mean_squared_error(labels = y, predictions = pred)\n","\n","# Gradient Descent\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n","display_step = 1\n","\n","# Initialize the variables (i.e. assign their default value)\n","init = tf.global_variables_initializer()\n","\n","# Re-run the training\n","train_session(train, test, y_train, y_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UH9NriegeiUg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# If you have time: \n","# Change the learning rate.\n","# Change the cost function.\n","# What's the best accuracy you've got? "],"execution_count":0,"outputs":[]}]}